# -*- coding: utf-8 -*-
"""NLP_News Classification TensorFlow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IDQt830sjWB85n2rnbGLDbMpOS169BwR

***About Dataset***

Dataset yang digunakand dalam tugas ini adalah bersumber dari kaggel. Dataset ini merupakan dataset untuk multi label kategory berita BBC, bisa di akses dalam [Tautan](https://www.kaggle.com/datasets/sainijagjit/bbc-dataset) ini.

Data tersebut memiliki 5 label atau kelas (sport, business, politics, tech, dan entertainment). Total data ada 2225 baris dan 2 kolom, kolom label dan judul berita.

# Libraries
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('words')
nltk.download('omw-1.4')

from google.colab import files
import os
import re
import textblob
import json
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import tensorflow as tf
import matplotlib.pyplot as plt

"""# Download Dataset"""

# Upload file kaggle.json for permission download
files.upload()

os.listdir()

# Move file kaggle.jsonn to root folder
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
# set permissions
!chmod 600 /root/.kaggle/kaggle.json
# list all available datasets
# !kaggle datasets list

!kaggle datasets download -d sainijagjit/bbc-dataset

!unzip bbc-dataset.zip

"""# Data Load"""

# Read dataset
read_data = pd.read_csv('bbc-text.csv')

read_data.info()

read_data.head()

read_data.info()

"""# Data Preprocessing

## Label Preprocessing
"""

# checking distribution of label
print(read_data['category'].value_counts())

#plot the distirbution
read_data['category'].value_counts().plot.bar()

# Get dummies data
category = pd.get_dummies(read_data.category)
df_baru = pd.concat([read_data, category], axis=1)
df_baru = df_baru.drop(columns='category')
df_baru.info()

# define label
label_news = df_baru.drop('text', axis=1).values
label_news

"""## Text/Attribute Preprocessing"""

!wget https://raw.githubusercontent.com/sholied/Dicoding-ProjectAkhir/main/dict.txt

# define contractions function
def contractions(text):
	# https://gist.github.com/nealrs/96342d8231b75cf4bb82
	# https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions

	cDict = json.load(open("dict.txt"))
	try:
		c_re = re.compile('(%s)' % '|'.join(cDict.keys()))

		def expandContractions(text, c_re=c_re):
			def replace(match):
				return cDict[match.group(0)]
			return c_re.sub(replace, text)
		
		text = expandContractions(text.lower())
		return text
	except Exception as e:
		print("contractions", e)

# remove stopwrods
stopwords_en = nltk.corpus.stopwords.words('english')
def remove_stopwords(text):
  result = []
  for token in text:
      if token not in stopwords_en:
          result.append(token)
  return result

# stemming
def stemming(text):
  porter = nltk.stem.PorterStemmer()
  result=[]
  for word in text:
      result.append(porter.stem(word))
  return result

# lemmatization
def lemmatization(text): 
  result=[]
  wordnet = nltk.stem.WordNetLemmatizer()
  for token,tag in nltk.pos_tag(text):
      pos=tag[0].lower()     
      if pos not in ['a', 'r', 'n', 'v']:
          pos='n'         
      result.append(wordnet.lemmatize(token,pos))
  return result

# make preprocessing text (remove urls, hastags, mention, etc) function
def textPreprocessing(text):
  try:
    # contractions
    text = contractions(text)
    # remove URLs
    text = re.sub('https?://[A-Za-z0-9./?&=_]+','',text)
    # hashtags
    text = re.sub('#[A-Za-z0-9]+','',text)
    # mentions
    text = re.sub('@[A-Za-z0-9._-]+','',text)
    #remove white spaces
    text = " ".join(text.strip().split())
    #lowercasing
    text = text.lower()
  except Exception as e:
    print("clearText error - ", e)

  return text

df_baru.info()

# preprocessing text 
df_baru.text=df_baru.text.apply(textPreprocessing)
# tokenization
df_baru.text=df_baru.text.apply(lambda X: nltk.word_tokenize(X))
# Stopwords
df_baru.text=df_baru.text.apply(remove_stopwords)
# lemmatization
df_baru.text=df_baru.text.apply(lemmatization)
# Steamming
df_baru.text=df_baru.text.apply(stemming)

df_baru

content = df_baru.text.values

# Tokenizer define
maxlen = 200
tokenizer = Tokenizer(num_words=5000, oov_token='-')
tokenizer.fit_on_texts(content)
sekuens = tokenizer.texts_to_sequences(content)
paddedset = pad_sequences(sekuens, maxlen=maxlen)
vocab_size = len(tokenizer.word_index)+1

paddedset.shape, label_news.shape

# split data into data train, validation (80 : 20)
data_latih, data_test, label_latih, label_test = train_test_split(paddedset, label_news, test_size=0.2, random_state=42)

n, num_class = label_latih.shape

"""# Model Training"""

# callback definition
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
                                                              filepath="/content/model.h5",
                                                              monitor='val_loss',
                                                              save_best_only=True)

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
                                                monitor='val_loss', 
                                                factor=0.2, 
                                                mode = 'min',
                                                patience=4, 
                                                min_lr=0.00000001)

#define custom early stopping when reach accuracy above 98%
class myCustomEarlyStopping(tf.keras.callbacks.Callback): 
    def on_epoch_end(self, epoch, logs={}): 
        if(logs.get('val_accuracy') > 0.90):
          print("\n Stop training, the model has reached {}% val accuracy!!".format(logs.get('val_accuracy')*100))   
          self.model.stop_training = True

early_stop = myCustomEarlyStopping()

callbacks = [model_checkpoint_callback, reduce_lr, [early_stop]]

# create model LSTM
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=50, input_length=200),
    tf.keras.layers.SpatialDropout1D(0.15),
    tf.keras.layers.LSTM(50, dropout=0.2),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(num_class, activation='softmax')
])
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
model.summary()

num_epochs = 30
history = model.fit(data_latih, label_latih, epochs=num_epochs, batch_size=10, 
                    validation_data=(data_test, label_test), verbose=1, callbacks = callbacks)

"""# Model Evaluation"""

def plot_history(history):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='Training acc')
    plt.plot(x, val_acc, 'r', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='Training loss')
    plt.plot(x, val_loss, 'r', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()

mymodel = tf.keras.models.load_model("/content/model.h5")
loss, accuracy = mymodel.evaluate(data_latih, label_latih, verbose=False)
print("Training Accuracy: {:.4f}".format(accuracy))
loss, accuracy = mymodel.evaluate(data_test, label_test, verbose=False)
print("Testing Accuracy:  {:.4f}".format(accuracy))
plot_history(history)